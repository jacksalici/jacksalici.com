---
title: "Gaze Estimation Dataset"
description: "A dataset for benchmarking gaze estimation"
layout: "../layouts/Page.astro"
---


<div class="bg-primary rounded-xl text-primary-content font-display text-xl p-5 mb-4">
I am recording a dataset aimed to leverage the human robot interaction research about gaze estimation. ðŸ‘€
</div>

I am looking for volunteers that want to help me for the last step of my M.Sc. thesis. If you are curious, take a look down below, ***thank you so much for the interest!***  

## The goal ðŸš€

The goal of my thesis was to create a framework for doing gaze estimation *in the wild*, that means not in laboratory. Using some nerdy goggles we can record where who wear them is looking at, and with my framework we can the gaze in a 3d space. Now we wants to record some data and elaborate them to create a dataset that can be used to benchmark other models and leverage the research in this field.  

Understanding where a person is looking at is fundamental in an huge amount of fields. In human robot interaction (HRI) gaze estimation represent a forced step to have robots more intelligent and more able to understand humans intentions.

![](/uploads/rerun.png)


## The hardware ðŸ¥½  

Both you and I will wear a pair of Project Aria Glasses like the one below. They can be worn as normal glasses.
The advantage of these devices is that they have 5 cameras: 2 internally for eye tracking, 2 for wide side-view, 1 for high-res view.  

![](/uploads/ariaglasses.png)


## What you'll do ðŸ¤–

***tldr:** Just look around. Seriously.*  

Here a bit longer version. You will be asked to *casually* look at things like objects, trees, but also a robot. On the other hand, I will be look at what you are you doing. Both you and I will wear Aria Glasses and will recording our gazes.  

In this way we will have a fist-person-view (FPV, your point of view) of the gazed objects, as well as an external view that will see the scene from a bit further. This second type images will be used for the dataset itself, while the first ones will be used of annotating the gaze position.  

### FAQ  

**How long it will take?**  
A recording session doesn't last more than a few minutes. Before we start I will briefly introduce you what we will done. The whole thing we last 20 minutes presumably.

**What do I get out?**  
Eternal gratitude, nothing more, nothing less.

**I wear prescription glasses. Can I participate?**  
Yes, it's not a problem. You will be asked to wear the Project Aria glasses during the recording, but during that time you don't have to read or move. You can gaze at random objects, the recording works also if they look blurred to you.

**What are you doing with my images?**  
The goal is to collect a dataset that will be used for gaze estimation research. Possibly it may be used also by other universities and research laboratories. Some images may be used in my thesis or in other future publications both mine or by others as well as in some topic-related websites. The images will never be sold commercially or in context other than research. It will be never appear your name or other data from you other than the images.  

You will be asked to sign a privacy policy.


## Who I am and how to contact me ðŸ“®

I am Giacomo (Jack) Salici, I am graduating in Computer Engineering M.Sc., AI curriculum. I should start a PhD in ICT later this year.  

If you are interested, please write me an email at `270385 AT studenti.unimore.it` or reach me on Telegram @ _jacksalici_.
